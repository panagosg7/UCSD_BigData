{
 "metadata": {
  "name": "",
  "signature": "sha256:2874d1d491a9748d012b4e80a547b459ec7806ffc8c8de71c67cf5a0b563c5ee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile pca.py\n",
      "#!/usr/bin/python\n",
      "\n",
      "\"\"\"\n",
      "<description here>\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "import gzip\n",
      "import pickle\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "class MRPCA(MRJob):\n",
      "\n",
      "    def configure_options(self):\n",
      "        super(MRPCA,self).configure_options()\n",
      "        # stations file\n",
      "        self.add_file_option('--stations')\n",
      "        \n",
      "        # groups to take into account\n",
      "        # this will automatically compute \n",
      "        self.add_passthrough_option('--groups', \n",
      "                                    type='string', \n",
      "                                    default='',\n",
      "                                    help='groups to take into account in comma separated form')\n",
      "        \n",
      "        self.add_passthrough_option('--key', type='string', default='')        \n",
      "        self.add_passthrough_option('--secret', type='string', default='')\n",
      "        \n",
      "     \n",
      "    def parse_cs_ints(self,s):\n",
      "         for s in s.split(','):\n",
      "            try:\n",
      "                yield int(s)\n",
      "            except Exception as e:\n",
      "                pass               \n",
      "\n",
      "\n",
      "        \n",
      "    def mapper_init(self):\n",
      "        f = gzip.open( self.options.stations, \"rb\" )\n",
      "        pickleFile = pickle.Unpickler( f )\n",
      "        self.stations = pickleFile.load()[['latitude','longitude','group_id']]\n",
      "        f.close()\n",
      "            \n",
      "        # valid groups\n",
      "        self.valid_groups = [ g for g in self.parse_cs_ints(self.options.groups) ]\n",
      "\n",
      "        # if a set of groups has been specified, then merge these \n",
      "        # groups into a single group and compute PCA for that\n",
      "        self.needs_merging = len(self.valid_groups) > 0\n",
      "            \n",
      "\n",
      "    \n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements = line.split(',')\n",
      "\n",
      "            # only include TMAX measurements\n",
      "            if elements[1] == 'TMAX':\n",
      "                \n",
      "                # station id\n",
      "                st_id = elements[0]\n",
      "\n",
      "                # group_id of the station \n",
      "                group_id = int(self.stations.loc[st_id]['group_id'])\n",
      "                \n",
      "                if self.needs_merging:                \n",
      "                \n",
      "                    # only include this if it belongs to a valid group\n",
      "                    if group_id in self.valid_groups:\n",
      "                        yield (0, ','.join(elements[3:]))                    \n",
      "                    \n",
      "                else:\n",
      "                    \n",
      "                    # pack just the content back into an comma separated string\n",
      "                    yield (group_id, ','.join(elements[3:]))\n",
      "\n",
      "                    \n",
      "        \n",
      "        except KeyError as e:\n",
      "            stderr.write('Key was not found.\\n')\n",
      "            \n",
      "        except Exception as e:\n",
      "            stderr.write('Error in line:\\n'+line)\n",
      "            #stderr.write(str(e))\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            \n",
      "            \n",
      "#     def combiner(self, key, values):\n",
      "#         self.increment_counter('MrJob Counters','combiner',1)                \n",
      "#         yield (key, values)\n",
      "\n",
      "\n",
      "    # PCA on the reducer\n",
      "    \n",
      "    def reducer(self, group_id, values):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)        \n",
      "\n",
      "        def to_int(s):\n",
      "            if s=='': \n",
      "                return np.NaN \n",
      "            else: \n",
      "                return int(s)\n",
      "        \n",
      "        vrows = [ v for v in values ]\n",
      "        \n",
      "        # FIXME: \n",
      "        # can we use `np.fromstring('s', dtype=int, sep=',')` here ?\n",
      "        # it seems that the absent value is translated into `0` instead of NaN                \n",
      "        Data=pd.DataFrame([map(to_int, v.split(',')) for v in vrows], columns=range(1,366))\n",
      "        \n",
      "        # Normalize data (from weather notebook)\n",
      "        G=Data.ix[:,1:365]\n",
      "        G[G<-400]=np.NaN\n",
      "        G[G>500]=np.NaN\n",
      "        G=G/10\n",
      "        Data.ix[:,1:365]=G\n",
      "        \n",
      "        from numpy import mean, std\n",
      "        \n",
      "        # Scale by Mean Std\n",
      "        matrix=Data.iloc[:,:]\n",
      "        Dout=Data.loc[:,range(1,366)]\n",
      "        Mean=mean(matrix, axis=1).values\n",
      "        Dout['Mean']=Mean\n",
      "        Std= std(matrix, axis=1).values\n",
      "        Dout['Std']=Std\n",
      "        Dout.loc[:,1:365]=matrix.values\n",
      "        \n",
      "\n",
      "        # Dout = Scale(Data) + 'mean' + 'std'\n",
      "        Dout=Dout[['Mean','Std']+range(1,366)]\n",
      "        \n",
      "        # Calculate covariance\n",
      "        M=Dout.loc[:,1:365].transpose()\n",
      "        M=M.dropna(axis=1)\n",
      "        (columns,rows)=M.shape\n",
      "        Mean=mean(M, axis=1).values\n",
      "        \n",
      "        C=np.zeros([columns,columns])   # Sum\n",
      "        N=np.zeros([columns,columns])   # Counter of non-nan entries\n",
      "        \n",
      "        for i in range(rows):\n",
      "            row = M.iloc[:,i]-Mean;\n",
      "            outer = np.outer(row,row)\n",
      "            valid = np.isnan(outer)==False\n",
      "            C[valid] = C[valid]+outer[valid]  # update C with the valid location in outer\n",
      "            N[valid] = N[valid]+1\n",
      "        cov = np.divide(C,N)\n",
      "        \n",
      "        # SVD\n",
      "        U,D,V=np.linalg.svd(cov)\n",
      "        \n",
      "        # explained variance\n",
      "        exp_var = np.cumsum(D[:])/np.sum(D)\n",
      "        \n",
      "        \n",
      "        # Dump some output\n",
      "        stderr.write('### reducer for group: ' + str(group_id) + ' no rows: ' \\\n",
      "                     + str(len(vrows)) + '\\n')\n",
      "            \n",
      "        \n",
      "        # Dump the eigenvectors at scratch bucket\n",
      "        # we transpose U so eigenvectors are on each row\n",
      "        eigpd = pd.DataFrame(U.T)\n",
      "\n",
      "        import StringIO\n",
      "        s = StringIO.StringIO()\n",
      "        eigpd.to_csv(s, index=False)\n",
      "\n",
      "\n",
      "        from boto.s3.connection import S3Connection\n",
      "        import boto\n",
      "        conn = boto.connect_s3(self.options.key, self.options.secret)\n",
      "        bucket = conn.get_bucket('pvekris.bucket')\n",
      "\n",
      "        from boto.s3.key import Key\n",
      "        k = Key(bucket)\n",
      "        \n",
      "        # group ids are unique so we are not going to overwrite any file\n",
      "        k.key = 'eigenvectors/eig_' + str(group_id) + '.csv'\n",
      "        k.set_contents_from_string(s.getvalue())   \n",
      "\n",
      "        \n",
      "        yield (group_id, ','.join([str(len(vrows))] + [str(i) for i in exp_var]))\n",
      "        \n",
      "        # output format\n",
      "        #\n",
      "        # <group_id> \"<no_of_measurement>,<e1>,<e2>,...,<e365>\"\n",
      "        #\n",
      "        # where ei is the explained covariance with i eigenvectors\n",
      "        \n",
      "if __name__ == '__main__':\n",
      "    MRPCA.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting pca.py\n"
       ]
      }
     ],
     "prompt_number": 355
    }
   ],
   "metadata": {}
  }
 ]
}