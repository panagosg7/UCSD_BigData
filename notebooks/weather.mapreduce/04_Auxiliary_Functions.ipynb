{
 "metadata": {
  "name": "",
  "signature": "sha256:a95fc038e940ff24ae97c0c1e5f002c9696d1d99e4ab4653e7fc1da118bc18d0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Auxiliary functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Method **pickle_dump** \"pickles\" object `obj` and dumps it in file `filename`.\n",
      "\n",
      "Method **pca_cmd** is a wrapper around the MapReduce call. It creates and executes commands of the form:\n",
      "\n",
      "    !python script.py -r emr --emr-job-flow-id XXXXXXXXXXXX hdfs:/weather/weather.csv > file.txt"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Merge neighboring groups\n",
      "\n",
      "The step of merging neighboring nodes consists of:\n",
      "\n",
      "1. Finding a *single* list of neighboring pairs.\n",
      "2. Exploring (via MapReduce PCA) these groupings.\n",
      "3. Comparing with existing groups using MDL.\n",
      "4. Performing the actual merging in the HI structure.\n",
      "\n",
      "\n",
      "### Explore possible group merges\n",
      "\n",
      "Given a list of neighboring groups on the map we will need to decide wether it makes \n",
      "sense to merge these groups or not. We will base our decision on the *Minimum Description\n",
      "Length* (*MDL*) criterion. \n",
      "\n",
      "In particular: suppose we are investigating wether it is worth\n",
      "merging groups $G_1$ and $G_2$. Group $G_1$ ($G_2$) contains $n_1$ ($n_2$) measurements and\n",
      "requires $k_1$ ($k_2$) vectors to reach $99\\%$ of the variance. The combined region contains \n",
      "$n_3 = n_1 + n_2$ measurements and requires $k_3$ vectors to reach $99\\%$ of the variance. It\n",
      "is easy to compute $n_3$ with the existing data, but at this point we cannot estimate $k_3$ \n",
      "precisely unless we run a new PCA analysis on the region $G_1 \\cup G_2$. So the $k_3$'s are \n",
      "computed by running \"exploratory\" MapReduce jobs on the temporarily merged groups. Once we \n",
      "have the necessary information, the citerion is the following:\n",
      "\n",
      "$$n_1 * k_1 + (k_1 + 1) * 365 + n_2 * k_2 + (k_2 + 1) * 365 < n_3 * k_3 + (k_3 + 1) * 365 $$\n",
      "\n",
      "This condition expresses our goal to move towards better representation of data in terms \n",
      "of space requirements. The break-down of these terms is:\n",
      "* $n_i * k_i$: size of the eigenvector coefficients\n",
      "* $(k_i+1) \\cdot 365$: size of the eigenvectors and mean value\n",
      "\n",
      "\n",
      "###Method description\n",
      "\n",
      "Method **`potential_pairs`** creates a list of pairs of neighboring groups, \n",
      "in the current state of the hierarchical index grouping `h`.\n",
      "\n",
      "**Note** that this pairing is a rather crude one. Each group can have an arbitrary number of \n",
      "neighbors, yet we are just going to explore one of many possible combinations of\n",
      "valid pairings of groups. Exploring more of these combination is potentially very \n",
      "expensive, so perhaps another heuristic can be used to pick among such neighboring pairs.\n",
      "\n",
      "\n",
      "Method **`explore`** launches a new batch of MapReduce PCA jobs, using as groups \n",
      "a set of potential pairings of the existing groups.\n",
      "\n",
      "####Possible Optimizaton:\n",
      "- Memoize results of PCA runs of previous explorations - since not all groups get updated on every run.\n",
      "\n",
      "Method **`get_mergeable_groups`** uses:\n",
      "* `main_results`: the information gathered by the latest run of the algorithm, and\n",
      "* `exploration_results`: the results of the exploration round (described above) \n",
      "\n",
      "It applies the MDL criterion to decide which groups are worth merging."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile misc.py\n",
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "home_dir='/home/ubuntu/panagosg7'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "print 'pandas version: ',pd.__version__\n",
      "print 'numpy version:',np.__version__\n",
      "print 'sklearn version:',sk.__version__\n",
      "import pickle\n",
      "import gzip\n",
      "\n",
      "from pylab import *\n",
      "\n",
      "\n",
      "def pickle_dump(obj, filename):\n",
      "    import gzip\n",
      "    import pickle\n",
      "    f = gzip.open(filename,'wb')\n",
      "    pickle.dump(obj,f)\n",
      "    f.close()\n",
      "\n",
      "    \n",
      "def pca_cmd(cmd_id, data, stations_file, outfile, emr=False, overwrite=True):\n",
      "    \n",
      "    # we need this for S3\n",
      "    import pickle\n",
      "    Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "    # print Creds.keys()\n",
      "    # print Creds['mrjob'].keys()\n",
      "    pair=Creds['mrjob']\n",
      "    key_id=pair['key_id']\n",
      "    secret_key=pair['secret_key']\n",
      "    ID=pair['ID']\n",
      "    # print ID,key_id\n",
      "    # do we need the keys in the command?\n",
      "    needs_keys = False\n",
      "    # main coommand\n",
      "    cmd = [ \"python pca.py\", data ]\n",
      "    # Are we running on emr?\n",
      "    if emr:\n",
      "        job_flow_id = find_waiting_flow(key_id,secret_key)\n",
      "        cmd = cmd + [ \"-r emr\", \"--emr-job-flow-id\", job_flow_id ]\n",
      "    # id \n",
      "    cmd = cmd + [ \"--id\", cmd_id ]\n",
      "    # stations' file\n",
      "    cmd = cmd + [ \"--stations\", stations_file ]\n",
      "    # creds' file\n",
      "    cmd = cmd + ['--creds', '/home/ubuntu/Vault/Creds.pkl']\n",
      "    # output\n",
      "    cmd  = cmd + [ '>', outfile ]\n",
      "    \n",
      "    # Make the command string\n",
      "    cmd_str = ' '.join(cmd)    \n",
      "    return cmd_str\n",
      "    \n",
      "    ## use the following when running inline\n",
      "    #!eval $cmd_str\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "def read_csv_from_s3(filename):\n",
      "    \"\"\"\n",
      "    filename needs to be relative to s3:/pvekris.bucket/    \n",
      "    Returns a dataframe containing the data in the file\n",
      "    \"\"\"\n",
      "    import pickle\n",
      "    Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "    pair=Creds['mrjob']\n",
      "    key_id=pair['key_id']\n",
      "    secret_key=pair['secret_key']\n",
      "    ID=pair['ID']\n",
      "    \n",
      "    from boto.s3.connection import S3Connection\n",
      "    import boto\n",
      "    from boto.s3.key import Key\n",
      "    import StringIO\n",
      "    import pandas as pd\n",
      "\n",
      "    conn = boto.connect_s3(key_id, secret_key)\n",
      "    bucket = conn.get_bucket('pvekris.bucket')\n",
      "    k = Key(bucket)\n",
      "    k.key = filename\n",
      "    string = k.get_contents_as_string()\n",
      "    stringIO = StringIO.StringIO(string)\n",
      "    return pd.read_csv(stringIO)\n",
      "\n",
      "\n",
      "\n",
      "def dump_matrix_to_s3(matrix, filename):\n",
      "    \"\"\"\n",
      "    filename needs to be relative to s3:/pvekris.bucket/\n",
      "    \"\"\"\n",
      "    import pickle\n",
      "    Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "    pair=Creds['mrjob']\n",
      "    key_id=pair['key_id']\n",
      "    secret_key=pair['secret_key']\n",
      "    ID=pair['ID']\n",
      "    \n",
      "    from boto.s3.connection import S3Connection\n",
      "    import boto\n",
      "    from boto.s3.key import Key\n",
      "    import StringIO\n",
      "    import pandas as pd\n",
      "\n",
      "    conn = boto.connect_s3(key_id, secret_key)\n",
      "    bucket = conn.get_bucket('pvekris.bucket')\n",
      "    k = Key(bucket)\n",
      "\n",
      "    k = Key(bucket)\n",
      "    df = pd.DataFrame(matrix)\n",
      "    import StringIO\n",
      "    s = StringIO.StringIO()\n",
      "    df.to_csv(s, index=False)\n",
      "\n",
      "    k.key = filename # 'cov_' + self.options.id + '/cov_' + str(group_id) + '.csv'\n",
      "    k.set_contents_from_string(s.getvalue())\n",
      "\n",
      "\n",
      "\n",
      "# Returns the combined covariance of the concatenation of two series of data with \n",
      "# covariance matrices c1 and c2, mean vectors m1 and m2 and counts n1 and n2.\n",
      "def tot_cov(c1,c2,n1,n2,m1,m2):\n",
      "    import numpy as np\n",
      "    c3 = np.divide( \n",
      "               (n1-1) * c1 + n1 * np.outer(m1,m1) + \n",
      "               (n2-1) * c2 + n2 * np.outer(m2,m2) - \n",
      "               np.divide(np.outer(n1*m1 + n2*m2, n1*m1 + n2*m2), n1+n2),\n",
      "               n1+n2-1)\n",
      "    \n",
      "    return c3\n",
      "\n",
      "def tot_mean(n1,n2,m1,m2):\n",
      "    import numpy as np\n",
      "    return np.divide(n1*m2 + n2*m2, n1+n2)\n",
      "\n",
      "\n",
      "\n",
      "## Test cum_cov\n",
      "\n",
      "# x = np.random.random(500*365).reshape(500,365)\n",
      "# y = np.random.random(500*365).reshape(500,365)\n",
      "# z = np.concatenate((x,y), axis=0)\n",
      "\n",
      "# m1 = np.mean(x,axis=0)\n",
      "# m2 = np.mean(y,axis=0)\n",
      "# m1.shape\n",
      "\n",
      "# c1 = np.cov(x.T)\n",
      "# c2 = np.cov(y.T)\n",
      "# c3 = np.cov(z.T)\n",
      "# c1.shape\n",
      "\n",
      "# (n1-1) * c1 + n1 * np.outer(m1,m1)\n",
      "# c1.shape\n",
      "\n",
      "# myc3 = cum_cov(c1,c2,500,500,m1,m2)\n",
      "\n",
      "# c3 - myc3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def make_pairs(h, gr=None):\n",
      "    existing_groups = h.get_groups()    \n",
      "    if gr == None:\n",
      "        group_keys = existing_groups.keys()\n",
      "    else:\n",
      "        group_keys = gr\n",
      "    \n",
      "    def pair_up():\n",
      "        # the seen nodes\n",
      "        p = set([])\n",
      "        for i in group_keys:\n",
      "            for j in group_keys:\n",
      "                if i < j and not (i in p) and not (j in p) and h.groups_are_neighbors(i,j):\n",
      "                    p.add(i)\n",
      "                    p.add(j)\n",
      "                    yield (i,j)\n",
      "                    \n",
      "    # potential new pairs\n",
      "    pairs = list(pair_up())\n",
      "        \n",
      "    return [(h.fresh_gid(), i, j) for (i,j) in pairs ]\n",
      "\n",
      "\n",
      "# parameter: the file name with the counts\n",
      "# returns { id: (n, mean, cov) }\n",
      "#\n",
      "def read_mr_results(suffix, filename):\n",
      "    import boto\n",
      "\n",
      "    import pandas as pd\n",
      "\n",
      "    print 'Reading map-reduce results '\n",
      "\n",
      "    counts = pd.DataFrame.from_csv(filename, sep='\\t', header=None)\n",
      "    counts.columns=['n']\n",
      "    data = {}\n",
      "    valid_groups = counts.index\n",
      "    \n",
      "    for (gid, nn) in counts.iterrows():\n",
      "        n = nn['n']\n",
      "        if n > 10:\n",
      "            try:            \n",
      "                s = 'cov_' + suffix + '/cov_' + str(gid) + '.csv'\n",
      "                cov_pd = read_csv_from_s3(s)\n",
      "\n",
      "                m = 'mean_' + suffix + '/mean_' + str(gid) + '.csv'\n",
      "                mean_pd = read_csv_from_s3(m)\n",
      "\n",
      "                sys.stdout.write('.')\n",
      "\n",
      "                data[gid] = (n, mean_pd, cov_pd.as_matrix())\n",
      "            except boto.exception.S3ResponseError as e:\n",
      "                print e\n",
      "    print\n",
      "    return data\n",
      "\n",
      "\n",
      "# data  :: { id: (n, mean, cov) } \n",
      "# returns: dataframe with stats (including k)\n",
      "# threshold is 85% of explained variance for k\n",
      "\n",
      "def do_PCA(data, do_plot=False):\n",
      "    \n",
      "    print 'PCA on', len(data.keys()), 'groups',\n",
      "\n",
      "    failed = []\n",
      "    Ps = []\n",
      "    import sys\n",
      "    import pandas as pd\n",
      "\n",
      "    for i in data.keys():\n",
      "        (n, mean, cov) = data[i]\n",
      "        try:\n",
      "            U,D,V=np.linalg.svd(cov)\n",
      "            sys.stdout.write('.')\n",
      "            p = np.cumsum(D[:])/sum(D)            \n",
      "            Ps.append(np.append(p,[i,n]))\n",
      "            \n",
      "            if do_plot:\n",
      "                plot(p)\n",
      "            \n",
      "        except Exception as e:\n",
      "            failed.append(i)\n",
      "            print e\n",
      "\n",
      "    Pdf = pd.DataFrame(Ps, columns=range(0,365) + ['gid','n']).set_index('gid')\n",
      "    Pdf['k'] = Pdf[Pdf[:] < 0.85].count(axis=1)\n",
      "\n",
      "    if do_plot:\n",
      "        xlim([0,100])\n",
      "        grid()\n",
      "        figure()\n",
      "    \n",
      "    print\n",
      "    \n",
      "    return (failed, Pdf)\n",
      "\n",
      "\n",
      "# returns (n3, m3, c3)\n",
      "def combine(suffix, old_groups, i1,i2):\n",
      "    s1 = 'cov_' + suffix + '/cov_' + str(i1) + '.csv'\n",
      "    c1 = read_csv_from_s3(s1)\n",
      "\n",
      "    s1 = 'mean_' + suffix + '/mean_' + str(i1) + '.csv'\n",
      "    m1 = read_csv_from_s3(s1)\n",
      "  \n",
      "    n1 = old_groups.loc[i1]['n']\n",
      "\n",
      "    s2 = 'cov_' + suffix + '/cov_' + str(i2) + '.csv'\n",
      "    c2 = read_csv_from_s3(s2)\n",
      "\n",
      "    s2 = 'mean_' + suffix + '/mean_' + str(i2) + '.csv'\n",
      "    m2 = read_csv_from_s3(s2)\n",
      "\n",
      "    n2 = old_groups.loc[i1]['n']\n",
      "    \n",
      "    return (n1+n2, tot_mean(n1,n2,m1,m2), tot_cov(c1,c2,n1,n2,m1,m2))\n",
      "\n",
      "\n",
      "# for a set of new pairings:\n",
      "# * compute new mean and covariance matrix - and dump them in s3\n",
      "# \n",
      "# returns potential pairings - non-crashing and new group dataframe info\n",
      "#\n",
      "def new_group_stats(suffix, old_groups, all_pairs, do_plot=False):\n",
      "    print 'Finding new group statistics ',\n",
      "    potential_pairs = []\n",
      "    newPs = []\n",
      "    new_data = {}\n",
      "\n",
      "    for (gid,i,j) in all_pairs:\n",
      "\n",
      "        (n3,m3,c3) = combine(suffix, old_groups, i,j)\n",
      "        sys.stdout.write('.')\n",
      "        try:\n",
      "            U,D,V=np.linalg.svd(c3)\n",
      "            p = np.cumsum(D[:])/sum(D)\n",
      "            \n",
      "            if do_plot:\n",
      "                plot(p, color='b')\n",
      "\n",
      "            newPs.append(np.append(p,[gid,n3]))\n",
      "            potential_pairs.append((gid,i,j))\n",
      "\n",
      "            #dump the statistics to s3\n",
      "            sc3 = 'cov_' + suffix + '/cov_' + str(gid) + '.csv'    \n",
      "            dump_matrix_to_s3(c3,sc3)\n",
      "\n",
      "            sm3 = 'mean_' + suffix + '/mean_' + str(gid) + '.csv'            \n",
      "            #print 'dumping', gid, 'to', sc3, 'and' , sm3    \n",
      "            dump_matrix_to_s3(m3,sm3)\n",
      "            new_data[gid] = (n3,m3,c3)\n",
      "\n",
      "        except Exception as e:\n",
      "            print e\n",
      "\n",
      "    new_groups = pd.DataFrame(newPs, columns=range(0,365) + ['gid','n']).set_index('gid')\n",
      "    new_groups['k'] = new_groups[new_groups[:] < 0.85].count(axis=1)\n",
      "    \n",
      "    if do_plot:\n",
      "        xlim([0,100])\n",
      "        grid()\n",
      "        figure()\n",
      "    \n",
      "    \n",
      "    print\n",
      "    \n",
      "    return (potential_pairs, new_groups, new_data)\n",
      "\n",
      "\n",
      "def mld_criterion(old_groups, new_groups, pairs):\n",
      "    \n",
      "    accepted_pairs = []\n",
      "\n",
      "    for (i3, i1, i2) in pairs:\n",
      "        n1 = old_groups.loc[i1]['n']\n",
      "        n2 = old_groups.loc[i2]['n']\n",
      "        n3 = new_groups.loc[i3]['n']\n",
      "\n",
      "        k1 = old_groups.loc[i1]['k']\n",
      "        k2 = old_groups.loc[i2]['k']\n",
      "        k3 = new_groups.loc[i3]['k']\n",
      "\n",
      "        # print n1,k1, n2,k2, n3,k3\n",
      "\n",
      "        A = n1*k1 + (k1+1)*365 + n2*k2 + (k2+1)*365\n",
      "        B = n3*k3 + (k3+1)*365\n",
      "\n",
      "        if A > B:\n",
      "            accepted_pairs.append((i3,i1,i2))\n",
      "    \n",
      "    return accepted_pairs\n",
      "\n",
      "\n",
      "# data  :: { id: (n, mean, cov) } \n",
      "def plot_expl_var(data):\n",
      "    for i in data.keys():\n",
      "        (n, mean, cov) = data[i]\n",
      "        try:\n",
      "            U,D,V=np.linalg.svd(cov)\n",
      "            p = np.cumsum(D[:])/sum(D)\n",
      "            plot(p, color='b')\n",
      "            \n",
      "        except Exception as e:\n",
      "            print e\n",
      "    \n",
      "    xlim([0,150])\n",
      "    grid()\n",
      "    show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting misc.py\n"
       ]
      }
     ],
     "prompt_number": 2
    }
   ],
   "metadata": {}
  }
 ]
}