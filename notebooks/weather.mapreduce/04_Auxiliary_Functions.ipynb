{
 "metadata": {
  "name": "",
  "signature": "sha256:28efaeecf2ded127d1b3602e6db503cb6939575b5e75c95a4233a3e84a1e4824"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Auxiliary functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Method **pickle_dump** \"pickles\" object `obj` and dumps it in file `filename`.\n",
      "\n",
      "Method **pca_cmd** is a wrapper around the MapReduce call. It creates and executes commands of the form:\n",
      "\n",
      "    !python script.py -r emr --emr-job-flow-id XXXXXXXXXXXX hdfs:/weather/weather.csv > file.txt"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile misc.py\n",
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "home_dir='/home/ubuntu/panagosg7'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "\n",
      "def pickle_dump(obj, filename):\n",
      "    import gzip\n",
      "    import pickle\n",
      "    f = gzip.open(filename,'wb')\n",
      "    pickle.dump(obj,f)\n",
      "    f.close()\n",
      "\n",
      "    \n",
      "def pca_cmd(cmd_id, data, stations_file, outfile, emr=False, overwrite=True):\n",
      "    \n",
      "    # we need this for S3\n",
      "    import pickle\n",
      "    Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "    # print Creds.keys()\n",
      "    # print Creds['mrjob'].keys()\n",
      "    pair=Creds['mrjob']\n",
      "    key_id=pair['key_id']\n",
      "    secret_key=pair['secret_key']\n",
      "    ID=pair['ID']\n",
      "    # print ID,key_id\n",
      "    \n",
      "    # do we need the keys in the command?\n",
      "    needs_keys = False\n",
      "\n",
      "    # main coommand\n",
      "    cmd = [ \"python pca.py\", data ]\n",
      "    \n",
      "    # Are we running on emr?\n",
      "    if emr:\n",
      "        job_flow_id = find_waiting_flow(key_id,secret_key)\n",
      "        cmd = cmd + [ \"-r emr\", \"--emr-job-flow-id\", job_flow_id ]\n",
      "    \n",
      "    # id \n",
      "    cmd = cmd + [ \"--id\", cmd_id ]\n",
      "    \n",
      "    # stations' file\n",
      "    cmd = cmd + [ \"--stations\", stations_file ]\n",
      "    \n",
      "    # creds' file\n",
      "    cmd = cmd + ['--creds', '/home/ubuntu/Vault/Creds.pkl']\n",
      "    \n",
      "    \n",
      "    #     # keys \n",
      "    #     if needs_keys:\n",
      "    #         cmd = cmd + [ \"--key\", key_id, \"--secret\", secret_key]\n",
      "    \n",
      "    # output\n",
      "    cmd  = cmd + [ '>', outfile ]\n",
      "    \n",
      "    # Make the command string\n",
      "    cmd_str = ' '.join(cmd)    \n",
      "    return cmd_str\n",
      "    \n",
      "    ## use the following when running inline\n",
      "    #!eval $cmd_str\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "def read_csv_from_s3(filename):\n",
      "    \"\"\"\n",
      "    filename needs to be relative to s3:/pvekris.bucket/\n",
      "    \n",
      "    Returns a dataframe containing the data in the file\n",
      "    \"\"\"\n",
      "    import pickle\n",
      "    Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "    pair=Creds['mrjob']\n",
      "    key_id=pair['key_id']\n",
      "    secret_key=pair['secret_key']\n",
      "    ID=pair['ID']\n",
      "    \n",
      "    from boto.s3.connection import S3Connection\n",
      "    import boto\n",
      "    from boto.s3.key import Key\n",
      "    import StringIO\n",
      "    import pandas as pd\n",
      "\n",
      "    conn = boto.connect_s3(key_id, secret_key)\n",
      "    bucket = conn.get_bucket('pvekris.bucket')\n",
      "    k = Key(bucket)\n",
      "    k.key = filename\n",
      "    string = k.get_contents_as_string()\n",
      "    stringIO = StringIO.StringIO(string)\n",
      "    return pd.read_csv(stringIO)\n",
      "\n",
      "\n",
      "\n",
      "def dump_matrix_to_s3(matrix, filename):\n",
      "    \"\"\"\n",
      "    filename needs to be relative to s3:/pvekris.bucket/\n",
      "    \"\"\"\n",
      "    import pickle\n",
      "    Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "    pair=Creds['mrjob']\n",
      "    key_id=pair['key_id']\n",
      "    secret_key=pair['secret_key']\n",
      "    ID=pair['ID']\n",
      "    \n",
      "    from boto.s3.connection import S3Connection\n",
      "    import boto\n",
      "    from boto.s3.key import Key\n",
      "    import StringIO\n",
      "    import pandas as pd\n",
      "\n",
      "    conn = boto.connect_s3(key_id, secret_key)\n",
      "    bucket = conn.get_bucket('pvekris.bucket')\n",
      "    k = Key(bucket)\n",
      "\n",
      "    k = Key(bucket)\n",
      "    df = pd.DataFrame(matrix)\n",
      "    import StringIO\n",
      "    s = StringIO.StringIO()\n",
      "    df.to_csv(s, index=False)\n",
      "\n",
      "    k.key = filename # 'cov_' + self.options.id + '/cov_' + str(group_id) + '.csv'\n",
      "    k.set_contents_from_string(s.getvalue())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Returns the combined covariance of the concatenation of two series of data with \n",
      "# covariance matrices c1 and c2, mean vectors m1 and m2 and counts n1 and n2.\n",
      "def tot_cov(c1,c2,n1,n2,m1,m2):\n",
      "    import numpy as np\n",
      "    c3 = np.divide( \n",
      "               (n1-1) * c1 + n1 * np.outer(m1,m1) + \n",
      "               (n2-1) * c2 + n2 * np.outer(m2,m2) - \n",
      "               np.divide(np.outer(n1*m1 + n2*m2, n1*m1 + n2*m2), n1+n2),\n",
      "               n1+n2-1)\n",
      "    \n",
      "    return c3\n",
      "\n",
      "def tot_mean(n1,n2,m1,m2):\n",
      "    import numpy as np\n",
      "    return np.divide(n1*m2 + n2*m2, n1+n2)\n",
      "\n",
      "\n",
      "\n",
      "## Test cum_cov\n",
      "\n",
      "# x = np.random.random(500*365).reshape(500,365)\n",
      "# y = np.random.random(500*365).reshape(500,365)\n",
      "# z = np.concatenate((x,y), axis=0)\n",
      "\n",
      "# m1 = np.mean(x,axis=0)\n",
      "# m2 = np.mean(y,axis=0)\n",
      "# m1.shape\n",
      "\n",
      "# c1 = np.cov(x.T)\n",
      "# c2 = np.cov(y.T)\n",
      "# c3 = np.cov(z.T)\n",
      "# c1.shape\n",
      "\n",
      "# (n1-1) * c1 + n1 * np.outer(m1,m1)\n",
      "# c1.shape\n",
      "\n",
      "# myc3 = cum_cov(c1,c2,500,500,m1,m2)\n",
      "\n",
      "# c3 - myc3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting misc.py\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Merge neighboring groups\n",
      "\n",
      "The step of merging neighboring nodes consists of:\n",
      "\n",
      "1. Finding a *single* list of neighboring pairs.\n",
      "2. Exploring (via MapReduce PCA) these groupings.\n",
      "3. Comparing with existing groups using MDL.\n",
      "4. Performing the actual merging in the HI structure.\n",
      "\n",
      "\n",
      "### Explore possible group merges\n",
      "\n",
      "Given a list of neighboring groups on the map we will need to decide wether it makes \n",
      "sense to merge these groups or not. We will base our decision on the *Minimum Description\n",
      "Length* (*MDL*) criterion. \n",
      "\n",
      "In particular: suppose we are investigating wether it is worth\n",
      "merging groups $G_1$ and $G_2$. Group $G_1$ ($G_2$) contains $n_1$ ($n_2$) measurements and\n",
      "requires $k_1$ ($k_2$) vectors to reach $99\\%$ of the variance. The combined region contains \n",
      "$n_3 = n_1 + n_2$ measurements and requires $k_3$ vectors to reach $99\\%$ of the variance. It\n",
      "is easy to compute $n_3$ with the existing data, but at this point we cannot estimate $k_3$ \n",
      "precisely unless we run a new PCA analysis on the region $G_1 \\cup G_2$. So the $k_3$'s are \n",
      "computed by running \"exploratory\" MapReduce jobs on the temporarily merged groups. Once we \n",
      "have the necessary information, the citerion is the following:\n",
      "\n",
      "$$n_1 * k_1 + (k_1 + 1) * 365 + n_2 * k_2 + (k_2 + 1) * 365 < n_3 * k_3 + (k_3 + 1) * 365 $$\n",
      "\n",
      "This condition expresses our goal to move towards better representation of data in terms \n",
      "of space requirements. The break-down of these terms is:\n",
      "* $n_i * k_i$: size of the eigenvector coefficients\n",
      "* $(k_i+1) \\cdot 365$: size of the eigenvectors and mean value\n",
      "\n",
      "\n",
      "###Method description\n",
      "\n",
      "Method **line_arrays** parses the lines of the MapReduce output.\n",
      "\n",
      "Method **`potential_pairs`** creates a list of pairs of neighboring groups, \n",
      "in the current state of the hierarchical index grouping `h`.\n",
      "\n",
      "**Note** that this pairing is a rather crude one. Each group can have an arbitrary number of \n",
      "neighbors, yet we are just going to explore one of many possible combinations of\n",
      "valid pairings of groups. Exploring more of these combination is potentially very \n",
      "expensive, so perhaps another heuristic can be used to pick among such neighboring pairs.\n",
      "\n",
      "\n",
      "Method **`explore`** launches a new batch of MapReduce PCA jobs, using as groups \n",
      "a set of potential pairings of the existing groups.\n",
      "\n",
      "####Possible Optimizaton:\n",
      "- Memoize results of PCA runs of previous explorations - since not all groups get updated on every run.\n",
      "\n",
      "Method **`get_mergeable_groups`** uses:\n",
      "* `main_results`: the information gathered by the latest run of the algorithm, and\n",
      "* `exploration_results`: the results of the exploration round (described above) \n",
      "\n",
      "It applies the MDL criterion to decide which groups are worth merging."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile merging.py\n",
      "#!/usr/bin/python\n",
      "\n",
      "def line_arrays(f):\n",
      "    \"\"\"\n",
      "    Parse lines of the form:\n",
      "    \n",
      "    <id>  \"<int>,<float>,<float>,...,<float>\"\n",
      "    \n",
      "    \"\"\"\n",
      "    import re\n",
      "    for line in f.readlines():\n",
      "        matchObj = re.match(r'\\s*(\\d*)\\s*\\\"(\\d*),(\\S*)\\\"\\s*', line)\n",
      "        i = matchObj.group(1)\n",
      "        n = matchObj.group(2)\n",
      "        s = matchObj.group(3)\n",
      "        yield np.array([int(i), int(n)] + [ float(s) for s in s.split(',') ])\n",
      "\n",
      "\n",
      "def make_pairs(h, gr=None):\n",
      "    existing_groups = h.get_groups()    \n",
      "    if gr == None:\n",
      "        group_keys = existing_groups.keys()\n",
      "    else:\n",
      "        group_keys = gr\n",
      "    \n",
      "    def pair_up():\n",
      "        # the seen nodes\n",
      "        p = set([])\n",
      "        for i in group_keys:\n",
      "            for j in group_keys:\n",
      "                if i < j and not (i in p) and not (j in p) and h.groups_are_neighbors(i,j):\n",
      "                    p.add(i)\n",
      "                    p.add(j)\n",
      "                    yield (i,j)\n",
      "                    \n",
      "    # potential new pairs\n",
      "    pairs = list(pair_up())\n",
      "        \n",
      "    return [(h.fresh_gid(), i, j) for (i,j) in pairs ]\n",
      "\n",
      "#     # first new key that needs to be created\n",
      "#     k = max(existing_groups.keys()) + 1\n",
      "\n",
      "#     # the new tentative group ids\n",
      "#     tmp_group_ids = list(xrange(k, k+len(pairs)+1))\n",
      "    \n",
      "#     gij = [ (g,i,j) for (g,(i,j)) in zip(tmp_group_ids, pairs) ]\n",
      "                            \n",
      "#     # print 'Potential pairs', gij\n",
      "#     # print\n",
      "#     return gij\n",
      "\n",
      "\n",
      "\n",
      "# explore new pairings\n",
      "def explore(h, pairing, input_data, output_file, emr):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    h             : H.I.\n",
      "    pairing       : list of (int,int,int) that we are going to explore\n",
      "    input_data    : \n",
      "    output_file   : PCA results will end up in `output_file`\n",
      "    emr           : run on EMR\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    output_file\n",
      "    \"\"\"\n",
      "    \n",
      "    # get a temp copy of the group stations  \n",
      "    tmp_stations = h.get_grouped_stations().copy(deep=True)\n",
      "\n",
      "    #print 'before adding temp groups'\n",
      "    #print 'tmp_stations[\\'group_id\\'].unique()'\n",
      "    #print tmp_stations['group_id'].unique()\n",
      "    \n",
      "    tmp_group_ids = map(lambda (x,y,z): x, pairing)\n",
      "\n",
      "    for (g,i,j) in pairing:\n",
      "        tmp_stations.loc[tmp_stations.group_id == i, 'group_id'] = g\n",
      "        tmp_stations.loc[tmp_stations.group_id == j, 'group_id'] = g \n",
      "    \n",
      "    #print 'after adding temp groups, ie. before pickling'\n",
      "    #print 'tmp_stations[\\'group_id\\'].unique()'\n",
      "    #print tmp_stations['group_id'].unique()\n",
      "\n",
      "    \n",
      "    # dump stations with group information\n",
      "    pickle_dump(tmp_stations, 'grouped_stations-tmp.pklz')\n",
      "\n",
      "    \n",
      "    pca_cmd(input_data, 'grouped_stations-tmp.pklz', tmp_group_ids, output_file, emr)\n",
      "    \n",
      "    return output_file\n",
      "\n",
      "\n",
      "\n",
      "def get_mergeable_groups(pairing, main_results, exploration_results):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    Each file contains lines of the form:\n",
      "        \n",
      "    <id>  \"<int>,<float>,<float>,...,<float>\"\n",
      "      \n",
      "    Returns\n",
      "    -------\n",
      "    list of (int,int) : a list of group ids that will be merged\n",
      "    \"\"\"\n",
      "    \n",
      "    # vector length\n",
      "    vlen = 365\n",
      "    \n",
      "    # results from main run\n",
      "    \n",
      "    table = [ l for l in line_arrays(open(main_results)) ]\n",
      "    cols  = ['group_id', 'n']+range(1,vlen+1)\n",
      "    \n",
      "    mdf    = pd.DataFrame(table, columns=cols).set_index('group_id')    \n",
      "\n",
      "    # find how many eignevectors are needed to explain at least 0.99 of the variance\n",
      "    mdf['k'] = vlen-sum(isnan(mdf[mdf<0.99].ix[:,1:vlen]),axis=1)\n",
      "\n",
      "    \n",
      "    # results from exploration run\n",
      "    \n",
      "    table = [ l for l in line_arrays(open(exploration_results)) ]\n",
      "    cols  = ['group_id', 'n3']+range(1,vlen+1)\n",
      "    \n",
      "    edf = pd.DataFrame(table, columns=cols).set_index('group_id')    \n",
      "\n",
      "    edf['i']  = -1\n",
      "    edf['j']  = -1    \n",
      "    edf['n1'] = -1 \n",
      "    edf['n2'] = -1 \n",
      "\n",
      "    # find how many eignevectors are needed to explain at least 0.99 of the variance\n",
      "    edf['k3'] = vlen-sum(isnan(edf[edf<0.99].ix[:,1:vlen]),axis=1)\n",
      "        \n",
      "    pairs_mapping = {}\n",
      "    for (g,i,j) in pairing:\n",
      "        edf.ix[g,'i'] = i\n",
      "        edf.ix[g,'j'] = j        \n",
      "        edf.ix[g,'n1'] = mdf.ix[i,'n']\n",
      "        edf.ix[g,'n2'] = mdf.ix[j,'n']\n",
      "        edf.ix[g,'k1'] = mdf.ix[i,'k']\n",
      "        edf.ix[g,'k2'] = mdf.ix[j,'k']\n",
      "        \n",
      "    \n",
      "    lhs = 'n1*k1+(k1+1)*365 + n2*k2+(k2+1)*365'\n",
      "    rhs = 'n3*k3+(k3+1)*365'\n",
      "\n",
      "    edf[lhs] = edf['n1']*edf['k1']+(edf['k1']+1)*vlen \\\n",
      "             + edf['n2']*edf['k2']+(edf['k2']+1)*vlen\n",
      "\n",
      "    edf[rhs] = edf['n3']*edf['k3']+(edf['k3']+1)*vlen\n",
      "            \n",
      "    edf['merge'] = edf[lhs] >= edf[rhs] \n",
      "    \n",
      "    # print edf[['i','j','n3','k3','n1','k1','n2', 'k2', lhs, rhs, 'merge']]\n",
      "\n",
      "    return [ (v['i'], v['j']) for (k,v) in edf.iterrows() if v['merge'] ]\n",
      "\n",
      "\n",
      "def merge_groups(h, input_data, main_results, output_dir, emr):\n",
      "    \"\"\"\n",
      "    Merge groups in hierarchical index `h` based on the results collected\n",
      "    from PCA on output file `filename`.\n",
      "    \n",
      "    Returns the list of newly created groups\n",
      "    \"\"\"\n",
      "    \n",
      "    # 1. Finding a *single* list of pairs of groups that can potentially be merged (are neighboring)\n",
      "    pairing = potential_pairs(h)\n",
      "    \n",
      "    # Use this file to dump the exploration output\n",
      "    if emr: \n",
      "        exploration_results = output_dir + 'pca_emr_tmp.txt'    \n",
      "    else:    \n",
      "        exploration_results = output_dir + 'pca_local_tmp.txt'    \n",
      "    \n",
      "    # 2. Exploring (via MapReduce PCA) these groupings\n",
      "    explore(h, pairing, input_data, exploration_results, emr)\n",
      "    \n",
      "    # 3. Comparing with existing groups using MDL.\n",
      "    merge_groups = get_mergeable_groups(pairing, main_results, exploration_results)\n",
      "    \n",
      "    # 4. Performing the actual merging in the HI structure.\n",
      "    result = []\n",
      "    for (i,j) in merge_groups:\n",
      "        new_g = h.merge_groups(i,j)   \n",
      "        if new_g >= 0:\n",
      "            print 'Merging groups:',i,j,'-->',new_g\n",
      "            result.append(new_g)\n",
      "            \n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting merging.py\n"
       ]
      }
     ],
     "prompt_number": 15
    }
   ],
   "metadata": {}
  }
 ]
}